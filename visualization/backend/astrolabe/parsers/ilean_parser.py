"""
Lean 4 .ilean Cache Parser

Reads declaration information from .lake/build/lib/lean/*.ilean files.
.ilean is a JSON index file generated by Lean compilation, containing:
- directImports: directly dependent modules
- module: module name
- references: definition and usage locations of all declarations

This is faster and more accurate than regex parsing.
"""

import json
import re
import logging
from pathlib import Path
from typing import Optional

from ..models.node import Node, NodeMeta, ProofStatus

logger = logging.getLogger(__name__)

# sorry detection regex: match standalone sorry keyword, excluding comments
SORRY_PATTERN = re.compile(r'\bsorry\b')
# line comment
LINE_COMMENT = re.compile(r'--.*$', re.MULTILINE)
# block comment
BLOCK_COMMENT = re.compile(r'/-.*?-/', re.DOTALL)


def parse_ilean_file(
    ilean_path: Path,
    project_root: Path,
    lazy_content: bool = False
) -> tuple[list[Node], list[str], dict]:
    """
    Parse a single .ilean file

    Args:
        ilean_path: .ilean file path
        project_root: project root directory (for locating source files)
        lazy_content: if True, don't read source code content (lazy loading)

    Returns:
        (nodes, imports, usage_map)
        - nodes: list of nodes
        - imports: list of imported modules
        - usage_map: {decl_full_name: [used_by_locations...]} for building edges
    """
    try:
        with open(ilean_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"[ilean] Error reading {ilean_path}: {e}")
        return [], [], {}

    module_name = data.get("module", "")
    direct_imports = data.get("directImports", [])
    references = data.get("references", {})

    # Extract imports (only take module names)
    imports = []
    for imp in direct_imports:
        if isinstance(imp, list) and len(imp) > 0:
            imports.append(imp[0])
        elif isinstance(imp, str):
            imports.append(imp)

    # Find the corresponding .lean source file
    source_file = find_source_file(module_name, project_root)

    # Extract declarations and usage information defined in this module from references
    nodes = []
    local_definitions = {}  # name -> definition info
    usage_map = {}  # full_name -> [(usage location line number, user declaration name), ...]

    for ref_key, ref_data in references.items():
        # Parse ref_key to get declaration information
        # Format: {"c":{"m":"Module.Name","n":"decl_name"}}
        try:
            ref_info = json.loads(ref_key)
            if "c" not in ref_info:
                continue

            decl_module = ref_info["c"].get("m", "")
            decl_name = ref_info["c"].get("n", "")
            full_name = f"{decl_module}.{decl_name}"

            definition = ref_data.get("definition")
            usages = ref_data.get("usages", [])

            # If it's a declaration in this module, record node information
            if definition is not None and decl_module == module_name:
                local_definitions[decl_name] = {
                    "definition": definition,
                    "usages": usages,
                    "full_name": full_name,
                }

            # Record usage information (for building edges)
            # usages format: [[line, col, ...], ...]
            # Some usages contain user information: [line, col, end_line, end_col, user_name, ...]
            if usages:
                usage_map[full_name] = []
                for usage in usages:
                    if len(usage) >= 5:
                        # Contains user name
                        user_name = usage[4]
                        usage_map[full_name].append({
                            "line": usage[0],
                            "user": user_name,
                            "user_module": module_name,
                        })
                    elif len(usage) >= 2:
                        # Only position, needs to be inferred later
                        usage_map[full_name].append({
                            "line": usage[0],
                            "user": None,
                            "user_module": module_name,
                        })

        except json.JSONDecodeError:
            continue

    # Read source file content (for getting declaration body and detecting sorry)
    # If lazy_content=True, skip source code reading
    source_content = ""
    source_lines = []

    if not lazy_content and source_file and source_file.exists():
        try:
            source_content = source_file.read_text(encoding="utf-8")
            source_lines = source_content.split("\n")
        except Exception:
            pass

    # Create nodes
    for name, info in local_definitions.items():
        definition = info["definition"]

        # definition format: [line, col, end_line, end_col, ...] or [line, col, end_line, end_col]
        # Note: This is only the position range of the name, not the entire declaration
        if len(definition) >= 4:
            line_start = definition[0]  # 0-indexed
        else:
            continue

        if lazy_content:
            # Lazy loading mode: don't read content, set status to unknown
            content = ""
            has_sorry = False
            kind = "unknown"  # Infer later when fetched via API
        else:
            # Extract the entire declaration from the beginning of the line (until the next top-level declaration or end of file)
            content = extract_full_declaration(source_lines, line_start)
            # Detect sorry (excluding comments)
            has_sorry = detect_sorry(content)
            # Infer declaration type
            kind = infer_kind(content)

        # Directly use the line number provided by .ilean (pointing to the name position, usually on the same line as the keyword)
        # Ensure line number is within valid range
        actual_line = min(line_start, len(source_lines) - 1) if source_lines else line_start

        node = Node(
            id=info["full_name"],
            name=name,
            kind=kind,
            file_path=str(source_file) if source_file else "",
            line_number=actual_line + 1,  # Convert to 1-indexed, use keyword line
            status=ProofStatus.SORRY if has_sorry else (ProofStatus.UNKNOWN if lazy_content else ProofStatus.PROVEN),
            references=[],  # Build later from usages
            _full_content=content if content else "",  # Internal use, for dependency analysis
        )
        nodes.append(node)

    return nodes, imports, usage_map


def find_source_file(module_name: str, project_root: Path) -> Optional[Path]:
    """
    Find the corresponding .lean source file based on module name

    Example: RamanujanNagell.Basic -> RamanujanNagell/Basic.lean
    """
    if not module_name:
        return None

    # Convert module name to path
    relative_path = module_name.replace(".", "/") + ".lean"

    # Try several possible locations
    candidates = [
        project_root / relative_path,
        project_root / "src" / relative_path,
    ]

    for candidate in candidates:
        if candidate.exists():
            return candidate

    return None


def extract_content(lines: list[str], line_start: int, col_start: int,
                   line_end: int, col_end: int) -> str:
    """
    Extract content from specified range in source code lines

    Note: Line numbers start from 0 (ilean format)
    """
    if not lines:
        return ""

    # Ensure line numbers are within range
    if line_start >= len(lines) or line_end >= len(lines):
        return ""

    if line_start == line_end:
        return lines[line_start][col_start:col_end]

    result = []
    for i in range(line_start, min(line_end + 1, len(lines))):
        if i == line_start:
            result.append(lines[i][col_start:])
        elif i == line_end:
            result.append(lines[i][:col_end])
        else:
            result.append(lines[i])

    return "\n".join(result)


# Top-level declaration keywords
DECL_KEYWORDS = {
    "theorem", "lemma", "def", "definition", "structure", "class",
    "instance", "axiom", "abbrev", "example", "inductive", "opaque"
}

# Lean 4 declaration modifiers that can precede keywords
DECL_MODIFIERS = {
    "noncomputable", "protected", "private", "partial", "unsafe", "scoped"
}


def find_decl_keyword_in_line(line: str) -> str | None:
    """
    Find a declaration keyword in a line, handling modifiers.

    Returns the keyword if found, None otherwise.
    Examples:
        "theorem foo" -> "theorem"
        "noncomputable def bar" -> "def"
        "protected theorem baz" -> "theorem"
        "@[simp] def qux" -> "def"
    """
    stripped = line.lstrip()
    if not stripped:
        return None

    words = stripped.split()

    for i, word in enumerate(words):
        # Skip attribute annotations like @[simp]
        if word.startswith("@["):
            continue
        # Skip modifiers
        if word in DECL_MODIFIERS:
            continue
        # Check if it's a declaration keyword
        if word in DECL_KEYWORDS:
            return word
        # If we hit something else, stop looking
        break

    return None


def detect_sorry(content: str) -> bool:
    """
    Detect if content contains sorry (excluding those in comments)

    Args:
        content: Lean code content

    Returns:
        True if contains sorry
    """
    if not content:
        return False

    # Remove comments
    content_clean = BLOCK_COMMENT.sub("", content)
    content_clean = LINE_COMMENT.sub("", content_clean)

    # Detect sorry
    return bool(SORRY_PATTERN.search(content_clean))


def find_declaration_by_name(lines: list[str], name: str, hint_line: int) -> int:
    """
    Search for declaration name in source file, find the line where "keyword name" is located

    Search pattern: theorem name, lemma name, def name, etc.
    This is more reliable than depending on .ilean's line number

    Args:
        lines: all lines in source file
        name: declaration name (e.g. sphere_eversion)
        hint_line: line number hint provided by .ilean (0-indexed), used for fallback

    Returns:
        line number where declaration is located (0-indexed)
    """
    if not lines or not name:
        return hint_line

    # Search for "keyword name" pattern
    for i, line in enumerate(lines):
        # Check if contains "keyword name" pattern
        for keyword in DECL_KEYWORDS:
            # Match "theorem name" or "theorem name:" or "theorem name ", etc.
            pattern = f"{keyword} {name}"
            if pattern in line or f"{keyword}  {name}" in line:
                return i

    # If not found, fall back to hint_line (ensure within valid range)
    return min(hint_line, len(lines) - 1) if lines else hint_line


def find_declaration_start(lines: list[str], line_hint: int) -> int:
    """
    Search upward from the line number provided by .ilean to find the actual line where declaration keyword is located

    The definition position provided by .ilean may be somewhere inside the declaration,
    need to search upward to find the line where theorem/lemma/def etc. keyword is located

    Handles modifiers like: noncomputable def, protected theorem, @[simp] lemma, etc.

    Args:
        lines: all lines in source file
        line_hint: line number provided by .ilean (0-indexed)

    Returns:
        line number where declaration keyword is located (0-indexed)
    """
    if not lines:
        return line_hint

    # If line number exceeds range, start searching from end of file
    start_line = min(line_hint, len(lines) - 1)

    # Search upward from start_line to beginning of file (no line limit)
    for i in range(start_line, -1, -1):
        line = lines[i]
        keyword = find_decl_keyword_in_line(line)
        if keyword is not None:
            return i

    # If keyword not found, return valid start line
    return start_line


def extract_full_declaration(lines: list[str], line_start: int) -> str:
    """
    Extract complete declaration content from starting line

    First search upward to find declaration keyword, then start extracting from there,
    until encountering the next top-level declaration or end of file

    Args:
        lines: all lines in source file
        line_start: starting line number (0-indexed), from .ilean

    Returns:
        complete declaration content
    """
    if not lines or line_start >= len(lines):
        return ""

    # Search upward to find the actual position of declaration keyword
    actual_start = find_declaration_start(lines, line_start)

    result = []

    for i in range(actual_start, len(lines)):
        line = lines[i]

        # After skipping the starting line, stop if encountering a new top-level declaration
        if i > actual_start:
            keyword = find_decl_keyword_in_line(line)
            if keyword is not None:
                break

        result.append(line)

    return "\n".join(result).strip()


def infer_kind(content: str) -> str:
    """
    Infer declaration type from content.

    Handles modifiers like: noncomputable, protected, private, partial, unsafe, scoped
    and attributes like: @[simp], @[inline], etc.

    Examples:
        "theorem foo : ..." -> "theorem"
        "noncomputable def bar : ..." -> "definition"
        "@[simp] protected lemma baz : ..." -> "lemma"
    """
    # Get the first line to find the keyword
    first_line = content.strip().split('\n')[0] if content else ""

    # Use the helper to find the keyword
    keyword = find_decl_keyword_in_line(first_line)

    if keyword is None:
        return "definition"  # fallback

    # Map keywords to kinds
    if keyword == "theorem":
        return "theorem"
    elif keyword == "lemma":
        return "lemma"
    elif keyword in ("def", "definition"):
        return "definition"
    elif keyword == "structure":
        return "structure"
    elif keyword == "class":
        return "class"
    elif keyword == "instance":
        return "instance"
    elif keyword == "axiom":
        return "axiom"
    elif keyword == "inductive":
        return "inductive"
    elif keyword == "abbrev":
        return "definition"
    elif keyword == "example":
        return "example"
    elif keyword == "opaque":
        return "opaque"
    else:
        return "definition"


def get_project_name(project_root: Path) -> str:
    """
    Get project name from lakefile.lean or lakefile.toml

    Args:
        project_root: project root directory

    Returns:
        project name (Lake package name)
    """
    # Try reading from lakefile.lean
    lakefile_lean = project_root / "lakefile.lean"
    if lakefile_lean.exists():
        try:
            content = lakefile_lean.read_text(encoding="utf-8")
            # Match name defined in lean_lib or lean_exe
            # Example: lean_lib StrongPNT or @[default_target] lean_lib StrongPNT
            import re
            match = re.search(r'lean_lib\s+(\w+)', content)
            if match:
                return match.group(1)
            # Or package name
            match = re.search(r'package\s+(\w+)', content)
            if match:
                return match.group(1)
        except Exception:
            pass

    # Try reading from lakefile.toml
    lakefile_toml = project_root / "lakefile.toml"
    if lakefile_toml.exists():
        try:
            content = lakefile_toml.read_text(encoding="utf-8")
            import re
            # Match [[lean_lib]] name = "..." or name = "..."
            match = re.search(r'\[\[lean_lib\]\][^\[]*name\s*=\s*["\'](\w+)["\']', content, re.DOTALL)
            if match:
                return match.group(1)
            # Or [package] name = "..."
            match = re.search(r'\[package\][^\[]*name\s*=\s*["\'](\w+)["\']', content, re.DOTALL)
            if match:
                return match.group(1)
        except Exception:
            pass

    # Fallback: use directory name
    return project_root.name


def parse_project_from_cache(
    project_root: Path,
    lazy_content: bool = False
) -> tuple[list[Node], list]:
    """
    Parse entire project from .lake/build cache

    Only scan project's own .ilean files, excluding dependency libraries:
    - .lake/packages/*  excluded
    - .lake/build/lib/lean/Mathlib/  excluded
    - Only scan .lake/build/lib/lean/{project_name}/

    Args:
        project_root: project root directory
        lazy_content: if True, don't read source code content (lazy loading)

    Returns:
        (nodes, edges)
        - nodes: all declaration nodes
        - edges: list of Edge objects
    """
    from ..models import Edge  # Avoid circular import

    lake_build = project_root / ".lake" / "build" / "lib" / "lean"

    if not lake_build.exists():
        print(f"[ilean] No build cache found at {lake_build}")
        return [], []

    # Get project name
    project_name = get_project_name(project_root)
    print(f"[ilean] Project name: {project_name}")

    # Only scan project's own .ilean files
    project_ilean_dir = lake_build / project_name

    if not project_ilean_dir.exists():
        # If project directory not found, try searching directly in lake_build for .ilean files matching project name
        print(f"[ilean] Project dir not found at {project_ilean_dir}, searching in {lake_build}")
        # List all directories under lake_build, excluding known dependency libraries
        excluded_dirs = {
            "Mathlib", "Batteries", "Aesop", "ProofWidgets", "Qq", "ImportGraph",
            "Lean", "Lake", "Init", "Std", "LeanSearchClient", "Plausible",
            "PrimeNumberTheoremAnd", "EulerProducts", "NumberTheory"
        }
        ilean_files = []
        for subdir in lake_build.iterdir():
            if subdir.is_dir() and subdir.name not in excluded_dirs:
                ilean_files.extend(subdir.rglob("*.ilean"))
                print(f"[ilean] Including directory: {subdir.name}")
    else:
        ilean_files = list(project_ilean_dir.rglob("*.ilean"))

    print(f"[ilean] Found {len(ilean_files)} project .ilean files (excluding dependencies)")

    # Only show main declaration types, exclude auxiliary definitions to improve performance
    # Includes all Lean 4 main declaration types
    MAIN_KINDS = {
        "theorem", "lemma", "axiom", "definition",
        "structure", "class", "instance", "inductive", "example"
    }

    all_nodes = []
    all_usage_maps = {}  # Merge usage_map from all files
    node_by_name = {}  # full_name -> node
    node_by_line = {}  # (module, line) -> node  for inferring user from line number

    for ilean_file in ilean_files:
        nodes, imports, usage_map = parse_ilean_file(ilean_file, project_root, lazy_content=lazy_content)

        # Get module name
        relative = ilean_file.relative_to(lake_build)
        if len(relative.parts) > 1:
            module_name = ".".join(p.replace(".ilean", "") for p in relative.parts)
        else:
            module_name = relative.stem

        # Filter: only keep theorems and lemmas
        filtered_nodes = [n for n in nodes if n.kind in MAIN_KINDS]

        for node in filtered_nodes:
            all_nodes.append(node)
            node_by_name[node.id] = node
            # Record line number to node mapping
            node_by_line[(module_name, node.line_number)] = node

        # Merge usage_map
        for target_name, usages in usage_map.items():
            if target_name not in all_usage_maps:
                all_usage_maps[target_name] = []
            all_usage_maps[target_name].extend(usages)

        print(f"[ilean] Parsed {ilean_file.name}: {len(filtered_nodes)}/{len(nodes)} theorems/lemmas")

    # Build edges (from usage_map)
    # usage_map records which locations each declaration is used at
    # We need to find which declaration the usage location belongs to, thus building source -> target edges
    edges = []
    seen_edges = set()  # Deduplication

    # Build line number range index: each node covers from its line to the next node's line
    # Used to infer user from usage line number
    node_ranges = {}  # module -> [(start_line, end_line, node_id), ...]
    for node in all_nodes:
        # Extract module name from node.id
        parts = node.id.rsplit(".", 1)
        if len(parts) == 2:
            mod = parts[0]
        else:
            mod = ""

        if mod not in node_ranges:
            node_ranges[mod] = []
        node_ranges[mod].append((node.line_number, node.id))

    # Sort by line number and calculate ranges
    for mod in node_ranges:
        node_ranges[mod].sort(key=lambda x: x[0])

    def find_node_at_line(module: str, line: int) -> Optional[str]:
        """Find the node at a given line number"""
        if module not in node_ranges:
            return None
        ranges = node_ranges[module]
        # Binary search or linear search
        for i, (start_line, node_id) in enumerate(ranges):
            # If after this node, before the next node
            if line >= start_line:
                if i + 1 < len(ranges):
                    if line < ranges[i + 1][0]:
                        return node_id
                else:
                    return node_id
        return None

    # Build edges from usage_map
    for target_id, usages in all_usage_maps.items():
        # Only process declarations in this project
        if target_id not in node_by_name:
            continue

        for usage in usages:
            user_module = usage.get("user_module", "")
            user_name = usage.get("user")
            usage_line = usage.get("line", 0) + 1  # Convert to 1-indexed

            if user_name:
                # Has explicit user name
                source_id = f"{user_module}.{user_name}"
            else:
                # Infer user from line number
                source_id = find_node_at_line(user_module, usage_line)

            if source_id and source_id in node_by_name:
                # source uses target, so edge is source -> target
                edge_key = (source_id, target_id)
                if edge_key not in seen_edges and source_id != target_id:
                    seen_edges.add(edge_key)
                    edges.append(Edge(
                        source=source_id,
                        target=target_id,
                        from_lean=True,
                    ))

    print(f"[ilean] Built {len(edges)} edges from cache")

    return all_nodes, edges
